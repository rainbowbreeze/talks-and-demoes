# Launch Ollama and Open-WebUI as two separate containers, in the same stack
# This makes both Ollama and Open-WebUI available in the network
#
# Launch with
#  docker compose -p "openwebui_stack" up -d
#  Without -p option, the stack takes the name of the source folder
# For logs
#  docker logs -f open-webui
#  docker logs -f ollama


services:
  # https://hub.docker.com/r/ollama/ollama
  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    restart: unless-stopped
    volumes:
      - /media/data/development/ollama/volumes/ollama:/root/.ollama
    # Ports directive can be removed if Ollama shouldn't be exposed outside of this stack
    ports:
      - 11434:11434
    tty: true
    environment:
      # All the Ollama env vars: https://github.com/ollama/ollama/blob/main/envconfig/config.go
      # Makes Ollama accessible outside of localhost, which is this specific container only
      - OLLAMA_HOST=0.0.0.0
      # Keeps model in cache for some more time
      - OLLAMA_KEEP_ALIVE=1h
      # Enable verbose logging
      - OLLAMA_DEBUG=1

    labels:
      # Watchtower will check for updates
      - "com.centurylinklabs.watchtower.enable=true"

    # Enable GPU Support (remove if no GPU are available)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]


  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:cuda
    restart: unless-stopped
    volumes:
      - /media/data/development/openweb-ui/volumes/open-webui:/app/backend/data
    ports:
      - 8080:8080
    depends_on:
      - ollama
    environment:
      # Enable debug level for Open-WebUI
      - GLOBAL_LOG_LEVEL=DEBUG
      # Access Ollama using its container name, because there are on the same stack
      #  and OLLAMA_HOST above makes possible to access Ollama outside of localhost
      - OLLAMA_BASE_URL=http://ollama:11434

    labels:
      # Watchtower will check for updates
      - "com.centurylinklabs.watchtower.enable=true"

    # Enable GPU Support (remove if no GPU are available)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
