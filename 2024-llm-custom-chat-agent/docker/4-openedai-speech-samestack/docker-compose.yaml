services:
  # https://hub.docker.com/r/ollama/ollama
  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    restart: always
    tty: true

    volumes:
      - /media/data/development/ollama/volumes/ollama:/root/.ollama

    ports:
      - 11434:11434

    environment:
      # All the Ollama env vars: https://github.com/ollama/ollama/blob/main/envconfig/config.go
      # Makes Ollama accessible across al the network (the llm-network defined above),
      #  and not only localhost (which means only the local machine)
      - OLLAMA_HOST=0.0.0.0
      # Keeps model in cache for some more time
      - OLLAMA_KEEP_ALIVE=1h
      # Enable verbose logging
      - OLLAMA_DEBUG=1

    labels:
      # Watchtower will check for updates
      - "com.centurylinklabs.watchtower.enable=true"

    # Enable CUDA support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]


  tts:
    # Support also coqui-tts, in addition to piper, gpu required, very slow on CPU only
    container_name: openedai-speech
    image: ghcr.io/matatonic/openedai-speech

    # Init some values via an env file, with a sample
    # available at https://github.com/matatonic/openedai-speech/blob/main/sample.env
    env_file:
      /media/data/development/openweb-ui/tts/speech.env

    # Or via env vars
    #environment:
    #  - 'TTS_HOME=voices'
    #  - 'HF_HOME=voices'
    #  - 'PRELOAD_MODEL=xtts'
    ports:
      - "8000:8000"
    volumes:
      - /media/data/development/openweb-ui/volumes/openedai-speech/voices:/app/voices
      - /media/data/development/openweb-ui/volumes/openedai-speech/config:/app/config
    restart: unless-stopped

    labels:
      # Watchtower will check for updates
      - "com.centurylinklabs.watchtower.enable=true"

    # Enable CUDA support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]


  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:cuda
    restart: unless-stopped
    volumes:
      - /media/data/development/openweb-ui/volumes/open-webui:/app/backend/data
    ports:
      - 8080:8080
    depends_on:
      - ollama
      - tts
    environment:
      # Enable debug level for Open-WebUI
      - GLOBAL_LOG_LEVEL=DEBUG
      # Access Ollama using its container name, because there are on the same stack
      #  and OLLAMA_HOST above makes possible to access Ollama outside of localhost
      - OLLAMA_BASE_URL=http://ollama:11434
      # The TTS engine 
      - AUDIO_TTS_OPENAI_API_BASE_URL=http://tts:8000/v1

    labels:
      # Watchtower will check for updates
      - "com.centurylinklabs.watchtower.enable=true"

    # Enable GPU Support (remove if no GPU are available)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
