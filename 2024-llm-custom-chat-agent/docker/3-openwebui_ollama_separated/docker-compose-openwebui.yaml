# Ollama runs on a container in the same host machine
#
# Launch with
#  docker compose -p "openwebui_stack" up -d
#  Without -p option, the stack takes the name of the source folder
# For logs
#  docker logs -f open-webui

services:
  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:cuda
    restart: unless-stopped

    volumes:
      - /media/data/development/openweb-ui/volumes/open-webui:/app/backend/data

    ports:
      - 8080:8080

    environment:
      # Enable debug level for Open-WebUI
      - GLOBAL_LOG_LEVEL=DEBUG
      # Ollama is a standalone container on the same machine
      - OLLAMA_BASE_URL=http://host.docker.internal:11434

    # Make the host computer available inside the container as host.docker.internal
    #  Because Ollama is installed as container in the host computer,
    #  host.docker.internal is a convenient alias to access the host computer,
    #  instead of specificying host computer IP address.
    #  localhost won't work because it refers to this container, no to the host computer
    extra_hosts:
      - host.docker.internal:host-gateway

    labels:
      # Watchtower will check for updates
      - "com.centurylinklabs.watchtower.enable=true"

    # Enable CUDA support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

